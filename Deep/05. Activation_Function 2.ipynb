{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Relu\n",
    "- By Using of Sigmoid And Tanh function there is vanishing gradient problem occure so the convergence rate slow down.\n",
    "- To overcome slighlty we use Relu Function. it not totally overcome this problem but here the convergence rate is faster than sigmoid and tanh.\n",
    "- Value Range :- [0, inf)\n",
    "- Its Nature non-linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.\n",
    "<img src=\"20.png\" style=\"height:350px;\">\n",
    "\n",
    "##### Point:\n",
    "1. <u>Vanishing Gradient Problem</u> occure due to multiple number of derivative.\n",
    "2. As sigmoid derivative 0 to 0.25 so by multiple by this sigmoid derivative the result might vanishing gradient as number of hidden layer increase.\n",
    "3. But in Relu here its derivative 0 to 1 so here no problem of any vanishing gradient problem as its derivative cant be 0.2,0.3 like. \n",
    "4. But as its derivative can be 0 so here a problem aries that called Dead_Activation\n",
    "\n",
    "<b>Uses :-</b> ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation.\n",
    "\n",
    "### What is Dead_Activation ?\n",
    "- When Derivative equal to 0 in Relu Then New Weight = Old Weight : <img src='21.png'>\n",
    "\n",
    "which is not good for any model.Here Weight Cant be update. it occure when value of z is negative.This state called Dead Activation state. to overcome this we use Leaky Relu.\n",
    "<img src=\"22.png\">\n",
    "- See here no updation happen the value remain same and ‘nan’ for validation loss is an unexpected very large or very small number. This is dead activation state to overcome this we use leaky relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leaky Relu\n",
    "- Leaky ReLU is an improved version of the ReLU function.\n",
    "- ReLU function, the gradient is 0 for x < 0(-ve), which made the neurons die for activations in that region.\n",
    "- Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for x less than 0, we define it as a small linear component of x. \n",
    "- Leaky ReLUs are one attempt to fix the Dying ReLU problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes:\n",
    "<img src=\"23.png\">\n",
    "<img src=\"24.png\"  style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid,Tanh,Relu,Leaky Relu\n",
    "\n",
    "<img src=\"26.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
